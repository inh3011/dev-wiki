# BitNet과 1.58비트 양자화 모델 정리

## 1. AI 모델에서 "숫자"의 의미는?

- AI 모델은 수많은 가중치(weight)라는 숫자로 구성됩니다.
- 이 가중치들은 입력을 처리하고 결과를 예측하는 데 쓰이는 모델의 지식입니다.
- 이 숫자는 컴퓨터에 비트(bit) 단위로 저장됩니다.

---

## 2. 비트 수는 왜 중요할까?

| 숫자 형식 | 비트 수 | 설명 |
| --- | --- | --- |
| float32 | 32비트 | 고정밀 실수 표현, 기본 딥러닝 형식 |
| float16 | 16비트 | 속도와 메모리를 줄인 형식 |
| int8/int4 | 8/4비트 | 정수 기반, 양자화에서 자주 사용 |
| BitNet | 1.58비트 (평균) | 평균적으로 1.58비트만 사용하는 초경량 표현 |
- 비트 수가 작아질수록 메모리 사용량은 줄고, 연산 속도는 빨라집니다.

---

## 3. 양자화(Quantization)란?

AI 모델의 가중치를 더 단순한 숫자(적은 비트)로 바꿔서, 모델을 더 작고 효율적으로 만드는 기술입니다.

양자화를 하면:

- 모델 파일 크기 감소
- 속도 증가
- 에너지 소비 감소
- 정밀도는 약간 손실될 수 있으나, 잘 튜닝하면 거의 차이 없음

---

## 4. BitNet의 1.58비트 양자화란?

BitNet은 모든 가중치를 다음 세 가지 값만 사용합니다.

```
복사편집
{-1, 0, +1}

```

- 이를 "Ternary Weight Representation"이라고 합니다.
- 이 3개 값을 가장 효율적으로 저장하면 평균 1.58비트 정도만 필요합니다.
- 실제로는 비트 수가 고정된 게 아니라, 전체 평균 비트 수가 1.58이라는 뜻입니다.

---

## 5. 1.58비트는 어떻게 가능할까?

BitNet은 다음과 같은 방법을 통해 평균 비트 수를 줄입니다:

1. 삼진값 사용: -1, 0, +1만 사용하여 표현 공간 최소화
2. 엔트로피 코딩: 자주 등장하는 값은 더 적은 비트로 압축
3. 블록 기반 인코딩: 여러 가중치를 묶어 함께 압축하여 비트 수 절감

이 조합으로 가중치 1개당 평균 1.58비트만 사용하게 됩니다.

---

## 6. 1.58비트 양자화 모델의 장점

| 항목 | 일반 모델 (float32) | BitNet (1.58bit) |
| --- | --- | --- |
| 메모리 사용량 | 매우 큼 (~4GB) | 매우 작음 (~0.2GB) |
| 속도 (CPU) | 느림 | 빠름 |
| 에너지 소비 | 큼 | 작음 |
| 정밀도 | 높음 | 거의 유사함 |

---

## 7. 실제 효과

- BitNet b1.58-2B 모델은 CPU 환경에서도 실시간 응답 수준의 속도를 보여줍니다.
- 메모리 사용량은 기존 모델 대비 최대 16배 절감됩니다.
- 에너지 소비는 최대 82% 절감됩니다.

---

## 8. 요약

BitNet은 모든 가중치를 -1, 0, +1 세 가지 값으로 제한하고, 압축 기술을 적용해 가중치 하나당 평균 1.58비트만 쓰는 초경량 LLM입니다.

그래서 GPU 없이도 CPU에서 빠르고 효율적으로 돌릴 수 있고, 모바일 및 엣지 디바이스 환경에서도 LLM의 가능성을 열어줍니다.
